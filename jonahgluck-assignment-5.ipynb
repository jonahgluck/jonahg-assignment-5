{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":9635558,"sourceType":"datasetVersion","datasetId":5883045}],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip3 install numba","metadata":{"execution":{"iopub.status.busy":"2024-10-17T19:20:25.982168Z","iopub.execute_input":"2024-10-17T19:20:25.983699Z","iopub.status.idle":"2024-10-17T19:20:38.870547Z","shell.execute_reply.started":"2024-10-17T19:20:25.983649Z","shell.execute_reply":"2024-10-17T19:20:38.869138Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Requirement already satisfied: numba in /opt/conda/lib/python3.10/site-packages (0.60.0)\nRequirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /opt/conda/lib/python3.10/site-packages (from numba) (0.43.0)\nRequirement already satisfied: numpy<2.1,>=1.22 in /opt/conda/lib/python3.10/site-packages (from numba) (1.26.4)\n","output_type":"stream"}]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom collections import Counter\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder, PolynomialFeatures\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.feature_selection import VarianceThreshold\nfrom sklearn.metrics import accuracy_score\nfrom numba import jit  # Import Numba for JIT compilation\n\n# Function to calculate Euclidean distance using Numba\n@jit(nopython=True)\ndef euclidean_distance(point1, point2):\n    return np.sqrt(np.sum((point1 - point2) ** 2))\n\n# Numba-optimized function to find k nearest neighbors\n@jit(nopython=True)\ndef find_k_nearest_neighbors(X_train, x, k):\n    distances = np.empty(X_train.shape[0])\n    for i in range(X_train.shape[0]):\n        distances[i] = euclidean_distance(x, X_train[i])\n    \n    k_indices = np.argsort(distances)[:k]\n    return k_indices\n\n# KNN class implementation\nclass KNN:\n    def __init__(self, k=3):\n        self.k = k\n\n    def fit(self, X_train, y_train):\n        self.X_train = X_train\n        self.y_train = y_train.to_numpy()  # Convert to NumPy array for indexing\n\n    def predict(self, X_test):\n        predictions = np.empty(X_test.shape[0], dtype=self.y_train.dtype)\n        for i in range(X_test.shape[0]):\n            predictions[i] = self._predict(X_test[i])\n        return predictions\n\n    def predict_proba(self, X_test):\n        probabilities = np.empty((X_test.shape[0], 2))  # Two classes: 0 and 1\n        for i in range(X_test.shape[0]):\n            probabilities[i] = self._predict_proba(X_test[i])\n        return probabilities\n\n    def _predict(self, x):\n        k_indices = find_k_nearest_neighbors(self.X_train, x, self.k)\n        k_nearest_labels = self.y_train[k_indices]  # Fetch labels of k nearest neighbors\n        most_common = Counter(k_nearest_labels).most_common(1)\n        return most_common[0][0]\n\n    def _predict_proba(self, x):\n        k_indices = find_k_nearest_neighbors(self.X_train, x, self.k)\n        k_nearest_labels = self.y_train[k_indices]\n        total = len(k_nearest_labels)\n        prob_1 = np.sum(k_nearest_labels) / total  # Probability of class 1\n        prob_0 = 1 - prob_1  # Probability of class 0\n        return np.array([prob_0, prob_1])  # Return probabilities for both classes\n\n# Load the train and test datasets\ntrain_path = '/kaggle/input/506-data/train.csv'\ntest_path = '/kaggle/input/506-data/test.csv'\n\ntrain_data = pd.read_csv(train_path)\ntest_data = pd.read_csv(test_path)\n\n# Drop irrelevant columns\ntrain_data = train_data.drop(['CustomerId', 'Surname'], axis=1)\ntest_data = test_data.drop(['CustomerId', 'Surname'], axis=1)\n\n# Split the train data into features and target\nX = train_data.drop(['Exited', 'id'], axis=1)  # Drop target and ID\ny = train_data['Exited']\n\n# Identify categorical columns (such as 'Geography', 'Gender')\ncategorical_cols = ['Geography', 'Gender']\n\n# Create a pipeline for preprocessing\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', StandardScaler(), X.select_dtypes(exclude=['object']).columns),  # Scale numeric features\n        ('cat', OneHotEncoder(), categorical_cols)  # One-hot encode categorical features\n    ])\n\n# Create polynomial features\npoly = PolynomialFeatures(interaction_only=True, include_bias=False)\n\n# Combine preprocessing steps into a pipeline\npipeline = Pipeline(steps=[\n    ('preprocessor', preprocessor),\n    ('poly', poly),\n    ('var_thresh', VarianceThreshold(threshold=0.1))  # Remove low-variance features\n])\n\n# Preprocess features and generate polynomial features\nX_transformed = pipeline.fit_transform(X)\n\n# Remove highly correlated features\ncorrelation_matrix = pd.DataFrame(X_transformed).corr().abs()\nhigh_correlation = np.where(correlation_matrix > 0.95)  # You can adjust the threshold\nto_drop = set()\n\nfor i in range(len(high_correlation[0])):\n    if high_correlation[0][i] != high_correlation[1][i]:\n        to_drop.add(high_correlation[1][i])\n\nX_filtered = np.delete(X_transformed, list(to_drop), axis=1)\n\n# Split the data into training and validation sets\nX_train, X_val, y_train, y_val = train_test_split(X_filtered, y, test_size=0.4, random_state=42)\n\n# Initialize and train the KNN model from scratch\nknn = KNN(k=5)  # You can adjust k\nknn.fit(X_train, y_train)\n\n# Make predictions on the validation set\ny_val_pred = knn.predict(X_val)\n\n# Calculate accuracy for both sets\ntrain_accuracy = accuracy_score(y_train, knn.predict(X_train))\nval_accuracy = accuracy_score(y_val, y_val_pred)\n\n# Print accuracy results\nprint(f'Training Accuracy: {train_accuracy * 100:.2f}%')\nprint(f'Validation Accuracy: {val_accuracy * 100:.2f}%')\n\n# Overfitting check\nif train_accuracy - val_accuracy > 0.1:  # 10% threshold for detecting overfitting, adjust as necessary\n    print(\"Warning: Possible overfitting detected.\")\nelse:\n    print(\"No significant overfitting detected.\")\n\n# Preprocess test data using the same pipeline (excluding the target)\nX_test = test_data.drop(['id'], axis=1)  # Drop ID\nX_test_transformed = pipeline.transform(X_test)\n\n# Remove the same highly correlated features from test data\nX_test_filtered = np.delete(X_test_transformed, list(to_drop), axis=1)\n\n# Make probability predictions on the test set\ny_test_prob = knn.predict_proba(X_test_filtered)\n\n# Prepare submission DataFrame\nsubmission = pd.DataFrame({\n    'id': test_data['id'],  # Include ID in the submission\n    'Exited': y_test_prob[:, 1]  # Probability of class 1 (Exited)\n})\n\n# Save to CSV\nsubmission.to_csv('submission.csv', index=False)\n\nprint(\"Submission file 'submission.csv' has been saved.\")\n","metadata":{"execution":{"iopub.status.busy":"2024-10-17T19:28:18.174996Z","iopub.execute_input":"2024-10-17T19:28:18.175459Z","iopub.status.idle":"2024-10-17T19:29:36.176714Z","shell.execute_reply.started":"2024-10-17T19:28:18.175410Z","shell.execute_reply":"2024-10-17T19:29:36.175511Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"Training Accuracy: 90.60%\nValidation Accuracy: 87.05%\nNo significant overfitting detected.\nSubmission file 'submission.csv' has been saved.\n","output_type":"stream"}]},{"cell_type":"code","source":"# Preprocess test data using the same pipeline (excluding the target)\nX_test = test_data.drop(['id'], axis=1)  # Drop ID\nX_test_transformed = pipeline.transform(X_test)\n\n# Remove the same highly correlated features from test data\nX_test_filtered = np.delete(X_test_transformed, list(to_drop), axis=1)\n\n# Make probability predictions on the test set\ny_test_prob = knn.predict_proba(X_test_filtered)\n\n# Prepare submission DataFrame\nsubmission = pd.DataFrame({\n    'id': test_data['id'],  # Include ID in the submission\n    'Exited': y_test_prob[:, 1]  # Probability of class 1 (Exited)\n})\n\n# Save to CSV\nsubmission.to_csv('submission.csv', index=False)\n\nprint(\"Submission file 'submission.csv' has been saved.\")\n","metadata":{"execution":{"iopub.status.busy":"2024-10-17T19:33:21.255569Z","iopub.execute_input":"2024-10-17T19:33:21.256104Z","iopub.status.idle":"2024-10-17T19:33:54.249172Z","shell.execute_reply.started":"2024-10-17T19:33:21.256052Z","shell.execute_reply":"2024-10-17T19:33:54.247968Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"Submission file 'submission.csv' has been saved.\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}